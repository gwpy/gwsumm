#!/usr/bin/env python
# coding=utf-8
# Copyright (C) Duncan Macleod (2013)
#
# This file is part of GWSumm.
#
# GWSumm is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# GWSumm is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GWSumm.  If not, see <http://www.gnu.org/licenses/>.

"""The gravitational-wave interferometer summary information system.

This module provides the command-line interface to the GWSumm package,
allowing generation of detector summary information.

Select a <mode> to run over a calendar amount of time ('day', 'week',
or 'month'), or an arbitrary GPS (semi-open) interval.

Run 'gw_summary <mode> --help' for details of the specific arguments and
options acceptable for each mode.
"""

from __future__ import (division, print_function)

import os
import datetime
import argparse
import calendar
import getpass
import warnings
import httplib
import importlib

from urlparse import urlparse

warnings.filterwarnings('ignore', 'TimeSeries.crop given GPS start')
warnings.filterwarnings('ignore', 'TimeSeries.crop given GPS end')
warnings.filterwarnings('ignore', category=RuntimeWarning)

from dateutil.relativedelta import relativedelta

try:
    import ROOT
except ImportError:
    pass
else:
    ROOT.PyConfig.IgnoreCommandLineOptions = True

from matplotlib import use
use('Agg')

# set matplotlib backend
try:
    from collections import OrderedDict
except ImportError:
    from astropy.utils import OrderedDict

from astropy import units

from glue.lal import Cache

try:
    from lal import lal
except ImportError:
    HASLAL = False
else:
    HASLAL = True

from gwpy.detector import Channel
from gwpy.segments import (Segment, SegmentList)
from gwpy.time import (tconvert, to_gps, Time)
from gwpy.spectrum import psd

from gwsumm import (globalv, version, mode, html)
from gwsumm.config import *
from gwsumm.channels import get_channels
from gwsumm.tabs import get_tab
from gwsumm.utils import *
from gwsumm.state import *
from gwsumm.data import get_timeseries_dict

__version__ = version.version
__author__ = 'Duncan Macleod <duncan.macleod@ligo.org>'

# XXX HACK: disable colon separator in ConfigParser
GWSummConfigParser.OPTCRE = re.compile(
    r'(?P<option>[^=\s][^=]*)\s*(?P<vi>[=])\s*(?P<value>.*)$')

# set defaults
VERBOSE = False
PROFILE = False
try:
    DEFAULT_IFO = get_default_ifo()
except ValueError:
    DEFAULT_IFO = None

# ----------------------------------------------------------------------------
# Argparse customisations

# find today's date
TODAY = datetime.datetime.utcnow().strftime('%Y%m%d')


# define custom parser
class GWArgumentParser(argparse.ArgumentParser):
    def __init__(self, *args, **kwargs):
        super(GWArgumentParser, self).__init__(*args, **kwargs)
        self._positionals.title = 'Positional arguments'
        self._optionals.title = 'Optional arguments'


# define custom help formatting (4-space)
class GWHelpFormatter(argparse.ArgumentDefaultsHelpFormatter):
    def __init__(self, *args, **kwargs):
        kwargs.setdefault('indent_increment', 4)
        super(GWHelpFormatter, self).__init__(*args, **kwargs)


# define actions for formatting dates
class DateAction(argparse.Action):
    TIMESCALE = {'days': 1}
    @staticmethod
    def set_gps_times(namespace, startdate, enddate):
        setattr(namespace, 'gpsstart', to_gps(startdate))
        setattr(namespace, 'gpsend', to_gps(enddate))

    def __call__(self, parser, namespace, values, option_string=None):
        try:
            date = datetime.datetime.strptime(values, self.DATEFORMAT)
        except ValueError:
            raise parser.error("%s malformed: %r. Please format as %s"
                               % (self.dest.title(), values, self.METAVAR))
        else:
            self.set_gps_times(namespace, date,
                               date + relativedelta(**self.TIMESCALE))
            setattr(namespace, self.dest, date)
        return date


class DayAction(DateAction):
    TIMESCALE = {'days': 1}
    DATEFORMAT = '%Y%m%d'
    METAVAR = 'YYYYMMDD'


class WeekAction(DayAction):
    TIMESCALE = {'days': 7}


class MonthAction(DateAction):
    TIMESCALE = {'months': 1}
    DATEFORMAT = '%Y%m'
    METAVAR = 'YYYYMM'


class YearAction(DateAction):
    TIMESCALE = {'years': 1}
    DATEFORMAT = '%Y'
    METAVAR = 'YYYY'


class GPSAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=False):
        try:
            values = float(values)
        except (TypeError, ValueError):
            pass
        setattr(namespace, self.dest, to_gps(values))

# ----------------------------------------------------------------------------
# Setup command-line parsing


# define top-level parser
parser = GWArgumentParser(
    formatter_class=GWHelpFormatter,
    description=__doc__,
    fromfile_prefix_chars='@',
    epilog="Arguments and options may be written into files and passed to "
           "%(prog)s as positional arguments prepended with '@', e.g. "
           "'%(prog)s @args.txt'. In this format, options must be give as "
           "'--argument=value', and not '--argument value'.")
parser.add_argument('-V', '--version', action='version',
                    version=__version__,
                    help="show program's version number and exit")

# define shared commands
sharedopts = GWArgumentParser(add_help=False)
sharedopts.title = 'Progress arguments'
sharedopts.add_argument('-v', '--verbose', action='store_true',
                        default=False, help="show verbose output")
sharedopts.add_argument('-D', '--debug', action='store_true',
                        default=False, help="show debug output")

# give configuration files
copts = sharedopts.add_argument_group("Configuration options",
                                      "Provide a number of INI-format "
                                      "configuration files")
copts.add_argument('-i', '--ifo', default=DEFAULT_IFO, metavar='IFO',
                   help="IFO prefix for interferometer to process. "
                        "If this option is set in the [DEFAULT] of any of "
                        "the INI files, giving it here is redundant.")
copts.add_argument('-f', '--config-file', action='append', type=str,
                   metavar='FILE', default=[],
                   help="INI file for analysis, may be given multiple times")
copts.add_argument('-t', '--process-tab', action='append', type=str,
                   help="process only this tab, can be given multiple times")

popts = sharedopts.add_argument_group("Process options",
                                      "Configure how this summary will be "
                                      "processed.")
popts.add_argument('--nds', action='store_true', default='guess',
                   help='use NDS as the data source')
popts.add_argument('--multi-process', action='store', type=int,
                   default=1, dest='multiprocess', metavar='N',
                   help="use a maximum of N parallel processes at any time")
popts.add_argument('-b', '--bulk-read', action='store_true', default=False,
                   help="read all data up-front at the start of the job, "
                        "rather than when it is needed for a tab")
popts.add_argument('-S', '--on-segdb-error', action='store', type=str,
                   default='raise', choices=['raise', 'ignore', 'warn'],
                   help="action upon error fetching segments from SegDB")
popts.add_argument('-G', '--on-datafind-error', action='store', type=str,
                   default='raise', choices=['raise', 'ignore', 'warn'],
                   help="action upon error querying for frames from the "
                        "datafind server, default: %(default)s")
popts.add_argument('--data-cache', action='append', default=[],
                   help='path to LAL-format cache of TimeSeries data files')
popts.add_argument('--event-cache', action='append', default=[],
                   help='path to LAL-format cache of event trigger files')
popts.add_argument('--segment-cache', action='append', default=[],
                   help='path to LAL-format cache of state or data-quality '
                        'segment files')

# ----------------------------------------------------------------------------
# Define sub-parsers

# add HTML options
def add_output_options(parser_):
    """Add outuput options to the subparser.

    This is only needed because argparse can't handle mutually exclusive
    groups in a parent parser handed to a subparser for some reason.
    """
    outopts = parser_.add_argument_group("Output options")
    outopts.add_argument('-o', '--output-dir', action='store', type=str,
                         metavar='DIR', default=os.curdir,
                         help="Output directory for summary information")
    htmlopts = outopts.add_mutually_exclusive_group()
    htmlopts.add_argument('-m', '--html-only', action='store_true',
                          default=False,
                          help="Generate container HTML and navigation only")
    htmlopts.add_argument('-n', '--no-html', action='store_true',
                          default=False,
                          help="Generate inner HTML and contents only, not "
                               "supporting HTML")


# define hierarchichal archiving choise
def add_archive_options(parser_):
    """Add archiving options to the subparser.

    This is only needed because argparse can't handle mutually exclusive
    groups in a parent parser handed to a subparser for some reason.
    """
    hierarchopts = parser_.add_argument_group('Archive options')
    hierarchchoice = hierarchopts.add_mutually_exclusive_group()
    hierarchchoice.add_argument(
        '-a', '--archive', metavar='FILE_TAG', default=False,
        const='GW_SUMMARY_ARCHIVE', nargs='?',
        help="Read archived data from, and write processed data to "
             "an HDF archive file written with the FILE_TAG. If not "
             "given, no archive will be used, if given with no file "
             "tag, a default of '%(const)s' will be used.")
    hierarchchoice.add_argument(
        '-d', '--daily-archive', metavar='FILE_TAG', default=False,
        const='GW_SUMMARY_ARCHIVE',
        nargs='?', help="Read data from the daily archives, with the "
                        "given FILE_TAG. If not given, daily archives will be "
                        "used, if given with no file tag, a default of "
                        "'%(const)s' will be used.")

# define sub-parser handler
subparsers = parser.add_subparsers(
    dest='mode', title='Modes',
    description='Note: all dates are defined with UTC boundaries.\n'
                'The valid modes are:')
subparser = dict()

# DAY mode
daydoc = """
Run %s over a full UTC day, and link this day to others with a calendar
built into the HTML navigation bar. In this mode you can also archive data
in HDF-format files to allow progressive processing of live data without
restarting from scratch every time.""" % parser.prog
subparser['day'] = subparsers.add_parser('day', description=daydoc,
                                         epilog=parser.epilog,
                                         parents=[sharedopts],
                                         formatter_class=GWHelpFormatter,
                                         help="Process one day of data")
subparser['day'].add_argument('day', action=DayAction, type=str, nargs='?',
                              metavar=DayAction.METAVAR, default=TODAY,
                              help="Day to process")
add_output_options(subparser['day'])

darchopts = subparser['day'].add_argument_group('Archive options',
                                                'Choose if, and how, to '
                                                'archive data from this run')
darchopts.add_argument('-a', '--archive', metavar='FILE_TAG',
                       default=False, const='GW_SUMMARY_ARCHIVE', nargs='?',
                       help="Read archived data from, and write processed data "
                            "to, an HDF archive file written with the "
                            "FILE_TAG. If not given, no archive will be used, "
                            "if given with no file tag, a default of "
                            "'%(const)s' will be used.")

# WEEK mode
subparser['week'] = subparsers.add_parser('week', parents=[sharedopts],
                                          epilog=parser.epilog,
                                          formatter_class=GWHelpFormatter,
                                          help="Process one week of data")
subparser['week'].add_argument('week', action=WeekAction, type=str,
                               metavar=WeekAction.METAVAR,
                               help="Week to process (given as starting day)")
add_output_options(subparser['week'])
add_archive_options(subparser['week'])

# MONTH mode
subparser['month'] = subparsers.add_parser('month', parents=[sharedopts],
                                           epilog=parser.epilog,
                                           formatter_class=GWHelpFormatter,
                                           help="Process one month of data")
subparser['month'].add_argument('month', action=MonthAction, type=str,
                                metavar=MonthAction.METAVAR,
                                help="Month to process")
add_output_options(subparser['month'])
add_archive_options(subparser['month'])

# and GPS mode
subparser['gps'] = subparsers.add_parser('gps', parents=[sharedopts],
                                         epilog=parser.epilog,
                                         formatter_class=GWHelpFormatter,
                                         help="Process GPS interval")
subparser['gps'].add_argument('gpsstart', action=GPSAction, type=str,
                              metavar='GPSSTART', help='GPS start time.')
subparser['gps'].add_argument('gpsend', action=GPSAction, type=str,
                              metavar='GPSEND', help='GPS end time.')
garchopts = subparser['gps'].add_argument_group('Archive options',
                                                'Choose if, and how, to '
                                                'archive data from this run')
garchopts.add_argument('-a', '--archive', metavar='FILE_TAG',
                       default=False, const='GW_SUMMARY_ARCHIVE', nargs='?',
                       help="Read archived data from, and write processed data "
                            "to, an HDF archive file written with the "
                            "FILE_TAG. If not given, no archive will be used, "
                            "if given with no file tag, a default of "
                            "'%(const)s' will be used.")

add_output_options(subparser['gps'])

# ----------------------------------------------------------------------------
# Parse command-line and sanity check

opts = parser.parse_args()

if opts.debug:
    warnings.simplefilter('error', DeprecationWarning)

# set verbose output options
globalv.VERBOSE = opts.verbose
#globalv.PROFILE = opts.verbose

# find all config files
opts.config_file = [os.path.expanduser(fp) for csv in opts.config_file for
                    fp in csv.split(',')]

# check segdb option
if not opts.on_segdb_error in ['raise', 'warn', 'ignore']:
    parser.error("Invalid option --on-segdb-error='%s'" % opts.on_segdb_error)

# read configuration file
config = GWSummConfigParser(dict_type=OrderedDict)
config.optionxform = str
if opts.ifo:
    config.set(DEFAULTSECT, 'ifo', opts.ifo)
config.set(DEFAULTSECT, 'user', getpass.getuser())
config.read(opts.config_file)
config.files = map(os.path.abspath, opts.config_file)

try:
    ifo = config.get(DEFAULTSECT, 'ifo')
except NoOptionError:
    ifo = None
finally:
    globalv.IFO = ifo

# interpolate section names
for section in config.sections():
    if section.startswith('%(ifo)s'):
        if not ifo:
            e =  InterpolationMissingOptionError(
                     'ifo', 'DEFAULT', '%(ifo)s', section)
            e.args = ('%s\n%s' % (str(e), "Please give --ifo on the command "
                      "line, or specify 'ifo = XX' in the [DEFAULT] section "
                      "of the INI file to use interpolation in [section] "
                      "names"),)
            raise e
        s2 = section.replace('%(ifo)s', ifo)
        config._sections[s2] = config._sections.pop(section)

# double-check week mode matches calendar setting
if opts.mode == 'week':
    if config.has_option("calendar", "start-of-week"):
        weekday = getattr(calendar,
                          config.get("calendar", "start-of-week").upper())
        if weekday != opts.week.timetuple().tm_wday:
            msg = ("Cannot process week starting on %s. The "
                   "'start-of-week' option in the [calendar] section "
                   "of the INI file specifies weeks start on %ss."
                   % (opts.week.strftime('%Y%m%d'),
                      config.get("calendar", "start-of-week")))
            raise parser.error(msg)

# record times in ConfigParser
span = Segment(opts.gpsstart, opts.gpsend)
utc = tconvert(opts.gpsstart)
config.set(DEFAULTSECT, 'gps-start-time', str(int(opts.gpsstart)))
config.set(DEFAULTSECT, 'gps-end-time', str(int(opts.gpsend)))
config.set(DEFAULTSECT, 'yyyy', utc.strftime('%Y'))
config.set(DEFAULTSECT, 'yy', utc.strftime('%y'))
config.set(DEFAULTSECT, 'mm', utc.strftime('%m'))
config.set(DEFAULTSECT, 'dd', utc.strftime('%d'))
config.set(DEFAULTSECT, 'yyyymm', utc.strftime('%Y%m'))
config.set(DEFAULTSECT, 'yyyymmdd', utc.strftime('%Y%m%d'))
config.set(DEFAULTSECT, 'duration', str(int(opts.gpsend - opts.gpsstart)))
if HASLAL:
    nleap = lal.GPSLeapSeconds(int(opts.gpsstart))
    config.set(DEFAULTSECT, 'leap-seconds', str(nleap))
    config.set(DEFAULTSECT, 'gps-start-time-noleap',
               str(int(opts.gpsstart) - nleap))
    config.set(DEFAULTSECT, 'gps-end-time-noleap',
               str(int(opts.gpsend) - nleap))

starttime = Time(float(opts.gpsstart), format='gps')
endtime = Time(float(opts.gpsend), format='gps')

# set mode and output directory
mode.set_mode(mode.MODE_ENUM[opts.mode.upper()])
try:
    path = mode.get_base(utc)
except ValueError:
    path = os.path.join('%d-%d' % (opts.gpsstart, opts.gpsend))

# set LAL FFT plan wisdom level
duration = min(globalv.NOW, opts.gpsend) - opts.gpsstart
if duration > 200000:
     psd.LAL_FFTPLAN_LEVEL = 3
elif duration > 40000:
     psd.LAL_FFTPLAN_LEVEL = 2
else:
     psd.LAL_FFTPLAN_LEVEL = 1

# set processing options
if opts.multiprocess == 1:
    opts.multiprocess = False

# set global html only flag
if opts.html_only:
    globalv.HTMLONLY = True

# -----------------------------------------------------------------------------
# Setup

vprint("""
------------------------------------------------------------------------------
Welcome to the GW summary information system command-line interface
------------------------------------------------------------------------------

This is process %d
You have selected %s mode.
Start time %s (%s)
End time: %s (%s)
Output directory: %s
""" % (os.getpid(), mode.MODE_NAME[mode.get_mode()],
       starttime.utc.iso, starttime.gps,
       endtime.utc.iso, endtime.gps,
       os.path.abspath(os.path.join(opts.output_dir, path))))

# -- Load plugins
# loading the module containing a plugin should 'register' the relevant
# Plot, or Tab, so they are accessible downstream
try:
    plugins = config.ndoptions('plugins')
except NoSectionError:
    pass
else:
    for plugin in plugins:
        importlib.import_module(plugin)

# Load custom units
try:
    customunits = config.nditems('units')
except NoSectionError:
    pass
else:
    new_ = []
    for unit, b in customunits:
        if b.lower() == 'dimensionless':
            b = ''
        new_.append(units.def_unit([unit], units.Unit(b)))
    units.add_enabled_units(new_)

if not opts.html_only:
    # parse channel grouns into individual sections
    for section in config.sections():
        if re.match('channels[-\s]', section):
            names = split_channels(config.get(section, 'channels'))
            items = dict(config.nditems(section, raw=True))
            items.pop('channels')
            for name in names:
                name = name.strip(' \n')
                if not config.has_section(name):
                    config.add_section(name)
                for key, val in items.iteritems():
                    if not config.has_option(name, key):
                        config.set(name, key, val)

    # read all channels
    raw = set()
    trend = set()
    for section in config.sections():
        try:
            m = Channel.MATCH.match(section).groupdict()
        except AttributeError:
            continue
        else:
            if not m['ifo']:
                continue
        if m['trend']:
            trend.add(section)
        else:
            raw.add(section)
    for group in [raw, trend]:
        try:
            newchannels = get_channels(group)
        except httplib.HTTPException:
            newchannels = []

        # read custom channel definitions
        for channel, section in zip(newchannels, group):
            for key, val in nat_sorted(config.nditems(section),
                                       key=lambda x: x[0]):
                key = re_cchar.sub('_', key.rstrip())
                if key.isdigit():
                    if not hasattr(channel, 'bits'):
                        channel.bits = []
                    while len(channel.bits) < int(key):
                        channel.bits.append(None)
                    if val.startswith('r"') or val.startswith('r\''):
                        val = eval(val)
                    channel.bits.append(val)
                else:
                    try:
                        setattr(channel, key, eval(val.rstrip()))
                    except NameError:
                        setattr(channel, key, val.rstrip())

# read states
try:
    load_states(config)
except NoSectionError:
    generate_all_state(*span)

# read caches
cache = {}
for key, var in zip(['datacache', 'trigcache', 'segmentcache'],
                    [opts.data_cache, opts.event_cache, opts.segment_cache]):
    if var:
        cache[key] = Cache()
        for fp in var:
            with open(fp, 'rb') as f:
                cache[key].extend(Cache.fromfile(f))
        cache[key] = cache[key].sieve(segment=span)

# build directories
mkdir(opts.output_dir)
os.chdir(opts.output_dir)
plotdir = os.path.join(path, 'plots')
mkdir(plotdir)

# -----------------------------------------------------------------------------
# Read Archive

if not hasattr(opts, 'archive'):
    opts.archive = False

if opts.html_only:
    opts.archive = False
    opts.daily_archive = False
elif opts.archive is True:
    opts.archive = 'GW_SUMMARY_ARCHIVE'

archives = []

if opts.archive:
    from gwsumm import archive
    archivedir = os.path.join(path, 'archive')
    mkdir(archivedir)
    opts.archive = os.path.join(archivedir, '%s-%s-%d-%d.hdf'
                                % (ifo, opts.archive, opts.gpsstart,
                                        opts.gpsend - opts.gpsstart))
    if os.path.isfile(opts.archive):
        archives.append(opts.archive)
    else:
        vprint("No archive found in %s, one will be created at the end.\n"
               % opts.archive)

# read daily archive for week/month/... mode
if hasattr(opts, 'daily_archive') and opts.daily_archive:
    from gwsumm import archive
    s = utc
    e = tconvert(opts.gpsend)
    while s < e:
        daybase = mode.get_base(s, mode=mode.SUMMARY_MODE_DAY)
        ds = tconvert(s)
        s += datetime.timedelta(days=1)
        de = tconvert(s)
        archivedir = os.path.join(daybase, 'archive')
        arch = os.path.join(archivedir, '%s-%s-%d-%d.hdf'
                            % (ifo, opts.daily_archive, ds, de-ds))
        if os.path.isfile(arch):
            archives.append(arch)
    # don't read any actual data
    cache['datacache'] = Cache()

for arch in archives:
    vprint("Reading archived data from %s..." % arch)
    archive.read_data_archive(arch)
    vprint(" Done.\n")

# -----------------------------------------------------------------------------
# Read HTML configuration

try:
    css = [cval for (key, cval) in config.items('html') if
           re.match('css\d+', key)]
except NoSectionError:
    css = html.get_css(ifo)
else:
    if not css:
        css = html.get_css(ifo)
try:
    javascript = [jval for (key, jval) in config.items('html') if
                  re.match('javascript\d+', key)]
except NoSectionError:
    javascript = html.get_js()
else:
    if not javascript:
        javascript = html.get_js()

# enable comments
try:
    globalv.HTML_COMMENTS_NAME = config.get('html', 'disqus-shortname')
except (NoOptionError, NoSectionError):
    pass

# find new ifo bases
ifobases = {}
try:
    bases_ = config.nditems('html')
except NoSectionError:
    pass
else:
    for key, val in config.nditems('html'):
        if re.search('-base\Z', key):
            ifobases[key.rsplit('-', 1)[0].title()] = val

# -----------------------------------------------------------------------------
# Read tabs

# read all tabs
alltabs = []
for section in filter(lambda n: re.match('tab[-\s]', n),
                      config.sections()):
    if not opts.process_tab or section[4:] in opts.process_tab:
        try:
            type_ = config.get(section, 'type')
        except NoOptionError:
            type_ = 'default'
        else:
            if not type_.startswith('archived-'):
                type_ = 'archived-%s' % type_
        DataTab = get_tab('archived-data')
        Tab = get_tab(type_)
        if issubclass(Tab, DataTab):
            tab = Tab.from_ini(config, section, plotdir=plotdir, path=path)
        else:
            tab = Tab.from_ini(config, section, path=path)
        alltabs.append(tab)

# sort tabs into hierarchical list
tabs = {}
# 1. Assume all tabs without parents are parents themselves
for tab in filter(lambda tab: tab.parent is None, alltabs):
    tabs[tab.name] = tab
# 2. All remaining tabs without a defined parent define that parent
# 3. Sort all tabs into their parent sets
for tab in filter(lambda tab: tab.parent is not None, alltabs):
    tabs.setdefault(tab.parent, get_tab('default')(tab.parent, *tab.span))
    tab.parent = tabs[tab.parent]
    tab.parent.add_child(tab)

tabs = tabs.values()

# sort tabs by 'Summary', then lower case only, then everything else
_sort_tabs = lambda tab: (
    (tab.shortname == 'Summary' and tab.parent is None) and 1 or
    tab.shortname == 'Summary' and 2 or
    'ODC' in tab.shortname and 3 or
    tab.shortname.islower() and tab.shortname.upper() or
    tab.shortname.lower())
tabs.sort(key=_sort_tabs)
for tab in tabs:
    tab.children.sort(key=_sort_tabs)
alltabs.sort(key=_sort_tabs, reverse=True)

# get URL from output directory
if 'public_html' in os.getcwd():
    urlbase = os.path.sep + os.path.join(
                  '~%s' % config.get(DEFAULTSECT, 'user'),
                  os.getcwd().split('public_html', 1)[1][1:])
    base = urlbase
# otherwise get URL from html config
elif ifo in ifobases:
    urlbase = urlparse(ifobases[ifo]).path
    base = urlbase
# otherwise let the write_html processor work it out on-the-fly
else:
    urlbase = None
    base = None

# write 404 error page
if not opts.no_html and urlbase:
    top = os.path.join(urlbase, path)
    four0four = get_tab('404')(span[0], span[1], parent=None, path=path,
                               index=os.path.join(path, '404.html'))
    four0four.write_html(css=css, js=javascript, tabs=tabs, ifo=ifo,
                         ifomap=ifobases, top=top, base=base,
                         writedata=not opts.html_only,
                         writehtml=not opts.no_html)
    url404 = os.path.join(urlbase, four0four.index)
    with open(os.path.join(path, '.htaccess'), 'w') as htaccess:
        print('Options -Indexes', file=htaccess)
        print('ErrorDocument 404 %s' % url404, file=htaccess)
        print('ErrorDocument 403 %s' % url404, file=htaccess)

# write config page
about = get_tab('about')(span[0], span[1], parent=None, path=path)
if not opts.no_html:
    mkdir(about.path)
    about.write_html(css=css, js=javascript, tabs=tabs, config=config.files,
                     ifo=ifo, ifomap=ifobases, about=about.index, base=base,
                     writedata=not opts.html_only,
                     writehtml=not opts.no_html)

# -----------------------------------------------------------------------------
# Process all tabs

# XXX: bulk data reading could optimise things
if opts.bulk_read and not opts.html_only:
    vprint("\n-------------------------------------------------\n")
    vprint("Reading all data in BULK...\n")
    allts = set()
    allsv = set()
    allflags = set()
    for tab in alltabs:
        snames = []
        for state in tab.states:
            snames.append(state.name)
            if state.definition:
                allflags.update(re_flagdiv.split(state.definition))
        # get all data defined for the 'all' state
        if ALLSTATE in snames:
            allts.update(tab.get_channels('timeseries', 'spectrogram',
                                          'spectrum', 'histogram'))
            allsv.update(tab.get_channels('statevector'))
            allflags.update(tab.get_flags('segments'))
        # or get data for plots defined over all states
        else:
            for plot in tab.plots:
                if plot.state is not None:
                    continue
                if plot.type in ['timeseries', 'spectrogram', 'spectrum',
                                 'histogram']:
                    allts.update(plot.channels)
                elif plot.type in ['statevector']:
                    allsv.update(plot.channels)
                elif plot.type in ['segments']:
                    allflags.update([f for cflag in plot.flags for f in
                                     re_flagdiv.split(cflag)[::2] if f])
    allseg = SegmentList([span])
    if len(allflags):
        vprint("%d data-quality flags identified for segment query from all "
               "tabs...\n" % len(allflags))
        get_segments(allflags, allseg, config=config, return_=False)
    if len(allts):
        vprint("%d channels identified for TimeSeries from all tabs...\n"
               % len(allts))
        get_timeseries_dict(allts, allseg,
                            config=config, nds=opts.nds,
                            multiprocess=opts.multiprocess, return_=False)
    if len(allsv):
        vprint("%d channels identified for StateVector from all tabs...\n"
               % len(allsv))
        get_timeseries_dict(allsv, allseg,
                            config=config, nds=opts.nds, statevector=True,
                            multiprocess=opts.multiprocess, return_=False)

for tab in alltabs:
    vprint("\n-------------------------------------------------\n")
    if tab.parent:
        name = '%s/%s' % (tab.parent.name, tab.name)
    else:
        name = tab.name
    if not opts.html_only and isinstance(tab, get_tab('archived-data')):
        vprint("Processing %s\n" % name)
        tab.process(config=config, nds=opts.nds,
                    multiprocess=opts.multiprocess,
                    segdb_error=opts.on_segdb_error,
                    datafind_error=opts.on_datafind_error, **cache)
    if not tab.hidden:
        mkdir(tab.href)
        page = tab.write_html(css=css, js=javascript, tabs=tabs, ifo=ifo,
                              ifomap=ifobases, about=about.index, base=base,
                              writedata=not opts.html_only,
                              writehtml=not opts.no_html)
    vprint("%s complete!\n" % (name))

# -----------------------------------------------------------------------------
# Finalise

if opts.archive:
    vprint("\n-------------------------------------------------\n")
    vprint("Writing data to archive...")
    archive.write_data_archive(opts.archive)
    vprint("Done. Archive written in\n%s\n" % os.path.abspath(opts.archive))

vprint("""
------------------------------------------------------------------------------
All done. Thank you.
------------------------------------------------------------------------------
""")
